#!/usr/bin/env python3
"""
å­¦æœ¯è®ºæ–‡æœç´¢è„šæœ¬
ä½¿ç”¨ arXiv API æœç´¢å­¦æœ¯è®ºæ–‡
"""
import json
import subprocess
from datetime import datetime
from pathlib import Path
from urllib.parse import quote_plus

# æœç´¢å…³é”®è¯
SEARCH_QUERIES = [
    "AI data governance automation",
    "data stewardship machine learning",
    "metadata extraction neural networks",
    "data quality AI benchmark",
    "DAMA framework",
]

def search_arxiv(query):
    """æœç´¢ arXiv"""
    try:
        url = f"https://export.arxiv.org/api/query?search_query=all:{quote_plus(query)}&max_results=50"
        result = subprocess.run(
            ["curl", "-s", url],
            capture_output=True,
            text=True,
            timeout=30
        )
        return result.stdout
    except Exception as e:
        return f"Error: {e}"

def parse_arxiv_results(xml_content):
    """è§£æ arXiv ç»“æœ"""
    import re
    papers = []
    
    entries = re.findall(r'<entry>(.*?)</entry>', xml_content, re.DOTALL)
    
    for entry in entries:
        title = re.search(r'<title>(.*?)</title>', entry, re.DOTALL)
        authors = re.findall(r'<name>(.*?)</name>', entry)
        summary = re.search(r'<summary>(.*?)</summary>', entry, re.DOTALL)
        published = re.search(r'<published>(.*?)</published>', entry)
        link = re.search(r'<id>(.*?)</id>', entry)
        
        title_text = title.group(1).strip().replace('\n', ' ') if title else "Unknown"
        summary_text = summary.group(1).strip().replace('\n', ' ')[:500] if summary else ""
        year = published.group(1)[:4] if published else "Unknown"
        
        papers.append({
            "title": title_text,
            "authors": authors,
            "year": year,
            "abstract": summary_text,
            "url": link.group(1) if link else "",
            "source": "arxiv"
        })
    
    return papers

def academic_search():
    """å­¦æœ¯æœç´¢ä¸»å‡½æ•°"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    today_str = datetime.now().strftime("%Y-%m-%d")
    
    print("=" * 60)
    print(f"ğŸ“š DeepSeeker Academic Search")
    print(f"{today_str} {timestamp.split()[1]}")
    print("=" * 60)
    
    all_papers = []
    source_counts = {"arxiv": 0}
    year_2024_plus = 0
    
    for query in SEARCH_QUERIES:
        print(f"\nğŸ“Œ {query}")
        
        arxiv_result = search_arxiv(query)
        if "Error" not in arxiv_result:
            arxiv_papers = parse_arxiv_results(arxiv_result)
            all_papers.extend(arxiv_papers)
            source_counts["arxiv"] += len(arxiv_papers)
            print(f"  âœ… arxiv: {len(arxiv_papers)}")
        else:
            print(f"  âŒ arxiv: æœç´¢å¤±è´¥")
    
    # ç»Ÿè®¡ 2024+ è®ºæ–‡
    for paper in all_papers:
        try:
            year = int(paper.get("year", 0))
            if year >= 2024:
                year_2024_plus += 1
        except:
            pass
    
    # å»é‡
    seen_titles = set()
    unique_papers = []
    for paper in all_papers:
        title_lower = paper["title"].lower()
        if title_lower not in seen_titles:
            seen_titles.add(title_lower)
            unique_papers.append(paper)
    
    total = len(unique_papers)
    
    print(f"\nğŸ“Š æ€»è®¡: {total} ç¯‡")
    print(f"ğŸ“š æ¥æº: {source_counts}")
    print(f"ğŸ†• 2024+: {year_2024_plus} ç¯‡")
    
    # æ˜¾ç¤ºç¤ºä¾‹è®ºæ–‡
    print(f"\nğŸ“„ ç¤ºä¾‹è®ºæ–‡:")
    for i, paper in enumerate(unique_papers[:3], 1):
        title = paper["title"][:60] + "..." if len(paper["title"]) > 60 else paper["title"]
        year = paper["year"]
        source = paper["source"]
        print(f"  â€¢ {title} [{source}] {year}")
    
    # ä¿å­˜åˆ° discoveries ç›®å½•
    discoveries_dir = Path.home() / ".config" / "deepseeker" / "discoveries"
    discoveries_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = discoveries_dir / f"papers_{today_str}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(unique_papers, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… è®ºæ–‡å·²ä¿å­˜è‡³: {output_file}")
    
    print("\n---OUTPUT_START---")
    result = {
        "status": "success",
        "total_papers": total,
        "sources": source_counts,
        "year_2024_plus": year_2024_plus,
        "papers": unique_papers[:10],
        "timestamp": timestamp
    }
    print(json.dumps(result, ensure_ascii=False, indent=2))
    print("---OUTPUT_END---")
    
    return result

if __name__ == "__main__":
    academic_search()
