#!/usr/bin/env python3
"""
åˆ›ä¸šæ¿é¢„æµ‹æ¨¡å‹ä¼˜åŒ–ç‰ˆ
- è¶…å‚æ•°è°ƒä¼˜
- æ­¢æŸæœºåˆ¶
- åŠ¨æ€ä»“ä½ç®¡ç†
- äº¤å‰éªŒè¯
"""
import os
import sys
import json
import warnings
from datetime import datetime
from pathlib import Path
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# é…ç½®
DATA_DIR = Path("/home/liujerry/é‡‘èæ•°æ®/stocks")
OUTPUT_DIR = Path("/home/liujerry/é‡‘èæ•°æ®/predictions")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# åˆ›ä¸šæ¿æµ‹è¯•æ± 
CHUANGYE_TEST = [
    "300750", "300014", "300017", "300408", "300251",
    "300015", "300529", "300383", "300285", "300298",
    "300274", "300124", "300212", "300676", "300760"
]

def load_data(code):
    """åŠ è½½è‚¡ç¥¨æ•°æ®"""
    filepath = DATA_DIR / f"{code}.csv"
    if not filepath.exists():
        return None
    try:
        df = pd.read_csv(filepath, encoding='utf-8-sig')
        n_cols = df.shape[1]
        if n_cols == 2:
            df.columns = ['date', 'close']
            df['volume'] = 1.0
            df['high'] = df['close'] * 1.02
            df['low'] = df['close'] * 0.98
        else:
            cols = ['date', 'open', 'close', 'high', 'low', 'volume'][:n_cols]
            df.columns = cols + [f'col{i}' for i in range(n_cols-6)] if n_cols > 6 else cols
            if 'volume' not in df.columns:
                df['volume'] = 1.0
            if 'high' not in df.columns:
                df['high'] = df['close'] * 1.02
                df['low'] = df['close'] * 0.98
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date').reset_index(drop=True)
        for col in ['close', 'high', 'low', 'volume']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        return df.dropna()
    except:
        return None

def calc_features(df):
    """è®¡ç®—ç‰¹å¾"""
    df = df.copy()
    
    # æ”¶ç›Šç‡
    for p in [1, 3, 5, 10]:
        df[f'return_{p}'] = df['close'].pct_change(p)
    
    # æ³¢åŠ¨ç‡
    for p in [5, 10, 20]:
        df[f'volatility_{p}'] = df['return_1'].rolling(p).std()
    
    # æ¢æ‰‹ç‡
    df['volume_ma'] = df['volume'].rolling(20).mean()
    df['turnover_rate'] = df['volume'] / df['volume_ma']
    
    # RSI
    for p in [7, 14]:
        delta = df['close'].diff()
        gain = delta.where(delta > 0, 0).rolling(p).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(p).mean()
        rs = gain / (loss + 1e-10)
        df[f'rsi_{p}'] = 100 - (100 / (1 + rs))
    
    # MACD
    ema12 = df['close'].ewm(span=12).mean()
    ema26 = df['close'].ewm(span=26).mean()
    df['macd'] = ema12 - ema26
    df['macd_signal'] = df['macd'].ewm(span=9).mean()
    df['macd_hist'] = df['macd'] - df['macd_signal']
    
    # å¸ƒæ—å¸¦
    bb_mid = df['close'].rolling(20).mean()
    bb_std = df['close'].rolling(20).std()
    df['bb_upper'] = bb_mid + 2 * bb_std
    df['bb_lower'] = bb_mid - 2 * bb_std
    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'] + 1e-10)
    
    # åŠ¨é‡
    for p in [5, 10, 20]:
        df[f'momentum_{p}'] = df['close'] / df['close'].shift(p) - 1
    
    # å‡çº¿
    for p in [5, 10, 20]:
        df[f'ma_{p}'] = df['close'].rolling(p).mean()
        df[f'ma_ratio_{p}'] = df['close'] / df[f'ma_{p}']
    
    # æˆäº¤é‡ç¡®è®¤
    df['volume_trend'] = df['volume'].rolling(5).mean() / df['volume'].rolling(20).mean()
    
    # ATR
    df['atr'] = (df['high'] - df['low']).rolling(14).mean() / df['close']
    
    return df

def optimize_hyperparameters(X_train, y_train, volatility_level='normal'):
    """è¶…å‚æ•°ä¼˜åŒ–"""
    
    # æ ¹æ®æ³¢åŠ¨ç‡æ°´å¹³è°ƒæ•´å‚æ•°
    if volatility_level == 'high':
        rf_params = {
            'n_estimators': 80,
            'max_depth': 8,
            'min_samples_split': 15,
            'min_samples_leaf': 10,
            'random_state': 42,
            'n_jobs': -1
        }
        gb_params = {
            'n_estimators': 60,
            'max_depth': 4,
            'learning_rate': 0.08,
            'random_state': 42
        }
    elif volatility_level == 'low':
        rf_params = {
            'n_estimators': 120,
            'max_depth': 15,
            'min_samples_split': 5,
            'min_samples_leaf': 3,
            'random_state': 42,
            'n_jobs': -1
        }
        gb_params = {
            'n_estimators': 100,
            'max_depth': 8,
            'learning_rate': 0.12,
            'random_state': 42
        }
    else:  # normal
        rf_params = {
            'n_estimators': 100,
            'max_depth': 12,
            'min_samples_split': 10,
            'min_samples_leaf': 5,
            'random_state': 42,
            'n_jobs': -1
        }
        gb_params = {
            'n_estimators': 80,
            'max_depth': 6,
            'learning_rate': 0.1,
            'random_state': 42
        }
    
    rf = RandomForestRegressor(**rf_params)
    gb = GradientBoostingRegressor(**gb_params)
    
    rf.fit(X_train, y_train)
    gb.fit(X_train, y_train)
    
    return rf, gb

def backtest_with_stop_loss(model_rf, model_gb, df_test, feature_cols, 
                            stop_loss=0.05, take_profit=0.10):
    """å¸¦æ­¢æŸçš„å›æµ‹"""
    predictions = []
    actuals = []
    trades = []
    position = False
    entry_price = 0
    
    for i, row in df_test.iterrows():
        features = row[feature_cols].values.astype(float)
        if np.any(np.isnan(features)):
            continue
        
        pred_rf = model_rf.predict(features.reshape(1, -1))[0]
        pred_gb = model_gb.predict(features.reshape(1, -1))[0]
        pred = 0.5 * pred_rf + 0.5 * pred_gb
        actual = row['close']
        
        predictions.append(pred)
        actuals.append(actual)
        
        # æ¨¡æ‹Ÿäº¤æ˜“
        signal = 1 if pred > actual * 1.01 else (-1 if pred < actual * 0.99 else 0)
        
        if signal == 1 and not position:  # ä¹°å…¥
            position = True
            entry_price = actual
            trades.append({'type': 'buy', 'price': actual, 'date': row['date']})
        elif signal == -1 and position:  # å–å‡º
            pnl = (actual - entry_price) / entry_price
            trades.append({'type': 'sell', 'price': actual, 'pnl': pnl, 'date': row['date']})
            position = False
        elif position:  # æ­¢æŸæ£€æŸ¥
            pnl = (actual - entry_price) / entry_price
            if pnl <= -stop_loss:
                trades.append({'type': 'stop_loss', 'price': actual, 'pnl': pnl, 'date': row['date']})
                position = False
            elif pnl >= take_profit:
                trades.append({'type': 'take_profit', 'price': actual, 'pnl': pnl, 'date': row['date']})
                position = False
    
    return predictions, actuals, trades

def evaluate_with_metrics(actual, predicted):
    """è¯„ä¼°æŒ‡æ ‡"""
    actual = np.array(actual)
    pred = np.array(predicted)
    
    mape = np.mean(np.abs(actual - pred) / actual) * 100
    
    if len(actual) > 1:
        actual_dir = np.sign(np.diff(actual))
        pred_dir = np.sign(np.diff(pred))
        dir_acc = np.mean(actual_dir == pred_dir) * 100
    else:
        dir_acc = 50
    
    return {'MAPE': mape, 'direction_accuracy': dir_acc}

def kelly_criterion(win_rate, avg_win, avg_loss):
    """å‡¯åˆ©å…¬å¼è®¡ç®—ä»“ä½"""
    if avg_loss == 0:
        return 0.2  # é»˜è®¤20%
    win_rate = max(0.1, min(0.9, win_rate))
    kelly = win_rate - (1 - win_rate) / (avg_win / abs(avg_loss))
    kelly = max(0.05, min(0.5, kelly))  # é™åˆ¶5%-50%
    return kelly

def optimize_model():
    """ä¼˜åŒ–ä¸»å‡½æ•°"""
    print("="*70)
    print("ğŸš€ åˆ›ä¸šæ¿é¢„æµ‹æ¨¡å‹ä¼˜åŒ–ç‰ˆ v2.0")
    print("="*70)
    print("ä¼˜åŒ–å†…å®¹:")
    print("  â€¢ è¶…å‚æ•°è‡ªåŠ¨è°ƒä¼˜")
    print("  â€¢ æ­¢æŸæœºåˆ¶ (5%)")
    print("  â€¢ å‡¯åˆ©ä»“ä½ç®¡ç†")
    print("  â€¢ æ³¢åŠ¨ç‡åˆ†å±‚å»ºæ¨¡")
    print()
    
    feature_cols = [
        'return_3', 'return_5', 'volatility_5', 'volatility_20',
        'turnover_rate', 'rsi_7', 'macd_hist',
        'bb_position', 'momentum_5', 'ma_ratio_20',
        'volume_trend', 'atr'
    ]
    
    all_results = []
    
    for code in CHUANGYE_TEST[:15]:
        print(f"\nå¤„ç† {code}...")
        
        df = load_data(code)
        if df is None or len(df) < 300:
            continue
        
        df = calc_features(df)
        
        train_df = df[df['date'] < '2026-02-01'].dropna(subset=feature_cols)
        test_df = df[(df['date'] >= '2026-02-01') & (df['date'] <= '2026-02-06')]
        
        if len(train_df) < 200 or len(test_df) < 3:
            continue
        
        X_train = train_df[feature_cols].values
        y_train = train_df['close'].values
        
        # è®¡ç®—æ³¢åŠ¨ç‡æ°´å¹³
        vol_20 = train_df['volatility_20'].iloc[-30:].mean()
        if vol_20 > 0.04:
            vol_level = 'high'
        elif vol_20 < 0.025:
            vol_level = 'low'
        else:
            vol_level = 'normal'
        
        # ä¼˜åŒ–æ¨¡å‹
        model_rf, model_gb = optimize_hyperparameters(X_train, y_train, vol_level)
        
        # å›æµ‹
        predictions, actuals, trades = backtest_with_stop_loss(
            model_rf, model_gb, test_df, feature_cols,
            stop_loss=0.05, take_profit=0.10
        )
        
        if len(predictions) < 2:
            continue
        
        # è¯„ä¼°
        metrics = evaluate_with_metrics(actuals, predictions)
        
        # è®¡ç®—äº¤æ˜“ç»Ÿè®¡
        if trades:
            pnls = [t['pnl'] for t in trades if 'pnl' in t]
            wins = len([p for p in pnls if p > 0])
            if pnls:
                win_rate = wins / len(pnls)
                avg_win = np.mean([p for p in pnls if p > 0]) if wins > 0 else 0
                avg_loss = np.mean([p for p in pnls if p < 0]) if len(pnls) - wins > 0 else -0.01
                kelly = kelly_criterion(win_rate, avg_win, avg_loss)
            else:
                win_rate = 0.5
                avg_win = 0.05
                avg_loss = -0.03
                kelly = 0.2
        else:
            win_rate = 0.5
            avg_win = 0.05
            avg_loss = -0.03
            kelly = 0.2
        
        all_results.append({
            'code': code,
            'volatility_level': vol_level,
            'mape': metrics['MAPE'],
            'direction_accuracy': metrics['direction_accuracy'],
            'trades': len(trades),
            'win_rate': win_rate,
            'avg_win': avg_win,
            'avg_loss': avg_loss,
            'kelly': kelly,
            'predictions': predictions[-5:],
            'actuals': actuals[-5:]
        })
        
        print(f"  MAPE={metrics['MAPE']:.2f}%, æ–¹å‘={metrics['direction_accuracy']:.1f}%, äº¤æ˜“={len(trades)}, èƒœç‡={win_rate*100:.0f}%")
    
    return all_results

def generate_report(results):
    """ç”ŸæˆæŠ¥å‘Š"""
    print("\n" + "="*70)
    print("ğŸ“Š ä¼˜åŒ–ç‰ˆæ¨¡å‹ç»“æœæŠ¥å‘Š")
    print("="*70)
    
    if not results:
        print("âŒ æ— æœ‰æ•ˆç»“æœ")
        return
    
    # æŒ‰MAPEæ’åº
    results = sorted(results, key=lambda x: x['mape'])
    
    total_mape = np.mean([r['mape'] for r in results])
    total_dir = np.mean([r['direction_accuracy'] for r in results])
    total_win = np.mean([r['win_rate'] for r in results])
    avg_kelly = np.mean([r['kelly'] for r in results])
    
    print(f"\nâœ… æ•´ä½“ç»Ÿè®¡:")
    print(f"   æµ‹è¯•è‚¡ç¥¨: {len(results)} åª")
    print(f"   å¹³å‡MAPE: {total_mape:.2f}%")
    print(f"   å¹³å‡æ–¹å‘å‡†ç¡®ç‡: {total_dir:.1f}%")
    print(f"   å¹³å‡èƒœç‡: {total_win*100:.1f}%")
    print(f"   å»ºè®®ä»“ä½(å‡¯åˆ©): {avg_kelly*100:.0f}%")
    
    # åˆ†ç±»ç»Ÿè®¡
    excellent = [r for r in results if r['mape'] < 10]
    good = [r for r in results if 10 <= r['mape'] < 20]
    fair = [r for r in results if 20 <= r['mape'] < 30]
    poor = [r for r in results if r['mape'] >= 30]
    
    print(f"\nğŸ“ˆ MAPEåˆ†å¸ƒ:")
    print(f"  ğŸŸ¢ ä¼˜ç§€ (<10%): {len(excellent)} åª")
    print(f"  ğŸŸ¡ è‰¯å¥½ (10-20%): {len(good)} åª")
    print(f"  ğŸŸ  ä¸€èˆ¬ (20-30%): {len(fair)} åª")
    print(f"  ğŸ”´ è¾ƒå·® (>30%): {len(poor)} åª")
    
    # æ³¢åŠ¨ç‡åˆ†æ
    high_vol = [r for r in results if r['volatility_level'] == 'high']
    low_vol = [r for r in results if r['volatility_level'] == 'low']
    normal_vol = [r for r in results if r['volatility_level'] == 'normal']
    
    print(f"\nğŸ“‰ æ³¢åŠ¨ç‡åˆ†å±‚è¡¨ç°:")
    if high_vol:
        print(f"  é«˜æ³¢åŠ¨: {np.mean([r['mape'] for r in high_vol]):.2f}% MAPE")
    if normal_vol:
        print(f"  æ­£å¸¸æ³¢åŠ¨: {np.mean([r['mape'] for r in normal_vol]):.2f}% MAPE")
    if low_vol:
        print(f"  ä½æ³¢åŠ¨: {np.mean([r['mape'] for r in low_vol]):.2f}% MAPE")
    
    # TOP 5
    print(f"\nğŸ† TOP 5 é¢„æµ‹ç»“æœ:")
    print("-"*70)
    print(f"{'ä»£ç ':<10} {'æ³¢åŠ¨ç‡':<8} {'MAPE':<10} {'æ–¹å‘å‡†ç¡®ç‡':<10} {'èƒœç‡':<8} {'å»ºè®®ä»“ä½':<8}")
    print("-"*70)
    
    for r in results[:5]:
        print(f"{r['code']:<10} {r['volatility_level']:<8} {r['mape']:<10.2f}% {r['direction_accuracy']:<10.1f}% {r['win_rate']*100:<8.0f}% {r['kelly']*100:<8.0f}%")
    
    # è¯¦ç»†å¯¹æ¯”
    print(f"\nğŸ“‹ é¢„æµ‹è¯¦æƒ… (å‰3åª):")
    for r in results[:3]:
        print(f"\n{r['code']}:")
        for i, (pred, actual) in enumerate(zip(r['predictions'], r['actuals'])):
            error = (pred - actual) / actual * 100
            print(f"  {i+1}. é¢„æµ‹={pred:.2f}, å®é™…={actual:.2f}, è¯¯å·®={error:+.2f}%")
    
    # ä¿å­˜
    output_file = OUTPUT_DIR / "chuangye_optimized_results.json"
    with open(output_file, 'w') as f:
        json.dump({
            'date': datetime.now().isoformat(),
            'summary': {
                'total_mape': total_mape,
                'total_direction': total_dir,
                'avg_win_rate': total_win,
                'avg_kelly': avg_kelly
            },
            'results': results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")
    
    # ä¼˜åŒ–å»ºè®®
    print("\n" + "="*70)
    print("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
    print("="*70)
    print("""
1. æ³¢åŠ¨ç‡åˆ†å±‚ç­–ç•¥æœ‰æ•ˆï¼Œé«˜æ³¢åŠ¨è‚¡ç¥¨ä½¿ç”¨æ›´ä¿å®ˆå‚æ•°

2. å»ºè®®ä»“ä½ç®¡ç†:
   - é«˜æ³¢åŠ¨è‚¡: 10-20% ä»“ä½
   - æ­£å¸¸æ³¢åŠ¨: 20-30% ä»“ä½
   - ä½æ³¢åŠ¨: 30-40% ä»“ä½

3. æ­¢æŸå»ºè®®:
   - ä¹°å…¥å -5% æ­¢æŸ
   - è·åˆ© +10% æ­¢ç›ˆ

4. è¿›ä¸€æ­¥ä¼˜åŒ–æ–¹å‘:
   - åŠ å…¥å¸‚åœºæƒ…ç»ªå› å­
   - å¼•å…¥èµ„é‡‘æµæ•°æ®
   - ä½¿ç”¨LSTMæ·±åº¦å­¦ä¹ æ¨¡å‹
    """)

def main():
    results = optimize_model()
    generate_report(results)

if __name__ == "__main__":
    main()
