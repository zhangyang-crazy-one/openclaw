#!/usr/bin/env python3
"""
Google Scholar å­¦æœ¯æœç´¢
ä½¿ç”¨ scholarly åŒ…ï¼ˆæœ¬åœ°çˆ¬å–ï¼‰æˆ– SerpApi
"""
import json
import time
import warnings
from datetime import datetime
from pathlib import Path

# SerpApi é…ç½®
SERPAPI_KEY = ""  # ä»ç¯å¢ƒå˜é‡è¯»å–


def search_scholarly(query, limit=10, timeout=30):
    """ä½¿ç”¨ scholarly åŒ…æœç´¢ Google Scholar"""
    try:
        from scholarly import scholarly
        warnings.filterwarnings('ignore')
        
        # æœç´¢å‡ºç‰ˆç‰©
        results = []
        search_gen = scholarly.search_pubs(query, citations=False)
        
        for i, paper in enumerate(search_gen):
            if i >= limit:
                break
            
            bib = paper.get('bib', {})
            
            result = {
                "title": bib.get('title', ''),
                "url": paper.get('pub_url', ''),
                "authors": bib.get('author', []),
                "year": bib.get('year', ''),
                "venue": bib.get('venue', ''),
                "abstract": bib.get('abstract', '')[:500],
                "citations": paper.get('citedby', 0),
                "engine": "Google Scholar (scholarly)",
            }
            results.append(result)
            
            if (i + 1) % 3 == 0:
                print(f"   ğŸ“„ å·²è·å– {i+1} ç¯‡...")
        
        return results, "Google Scholar"
        
    except Exception as e:
        return None, str(e)


def search_serpapi_google_scholar(query, limit=10):
    """ä½¿ç”¨ SerpApi æœç´¢ Google Scholar"""
    global SERPAPI_KEY
    
    if not SERPAPI_KEY:
        SERPAPI_KEY = __import__('os').environ.get("SERPAPI_KEY", "")
    
    if not SERPAPI_KEY:
        return None, "éœ€è¦ SERPAPI_KEY"
    
    try:
        import urllib.request
        import urllib.parse
        
        params = {
            "engine": "google_scholar",
            "q": query,
            "num": limit,
            "api_key": SERPAPI_KEY,
        }
        
        url = f"https://serpapi.com/search?{urllib.parse.urlencode(params)}"
        
        req = urllib.request.Request(url, headers={"User-Agent": "DeepSeeker/1.0"})
        
        with urllib.request.urlopen(req, timeout=30) as response:
            data = json.loads(response.read().decode())
            
            results = []
            for item in data.get("organic_results", []):
                results.append({
                    "title": item.get("title", ""),
                    "url": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                    "authors": item.get("publication_info", {}).get("authors", []),
                    "year": item.get("publication_info", {}).get("year", ""),
                    "citations": item.get("cited_by", {}).get("value", 0),
                    "engine": "Google Scholar (SerpApi)",
                })
            
            return results, "Google Scholar (SerpApi)"
    
    except Exception as e:
        return None, str(e)


def google_scholar_search(query, use_scholarly=True, use_serpapi=False, limit=10):
    """Google Scholar æœç´¢ä¸»å‡½æ•°"""
    print("=" * 70)
    print(f"ğŸ“ Google Scholar æœç´¢: {query}")
    print("=" * 70)
    
    all_results = []
    sources_tried = []
    
    # æ–¹æ³•1: scholarly åŒ…
    if use_scholarly:
        print("ğŸ” ä½¿ç”¨ scholarly (Google Scholar æœ¬åœ°çˆ¬å–)...")
        print("   âš ï¸ é€Ÿåº¦è¾ƒæ…¢ï¼Œéœ€è¦ 10-30 ç§’...")
        
        start = time.time()
        results, source = search_scholarly(query, limit=limit)
        elapsed = int(time.time() - start)
        
        if results:
            all_results.extend(results)
            sources_tried.append(f"scholarly: {len(results)} ç¯‡ ({elapsed}s)")
            print(f"   âœ… æˆåŠŸ! æ‰¾åˆ° {len(results)} ç¯‡ ({elapsed}s)")
        else:
            sources_tried.append(f"scholarly: {source}")
            print(f"   âš ï¸ ä¸å¯ç”¨: {source}")
        
        time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    # æ–¹æ³•2: SerpApi
    if use_serpapi:
        print("ğŸ” ä½¿ç”¨ SerpApi...")
        results, source = search_serpapi_google_scholar(query, limit)
        
        if results:
            all_results.extend(results)
            sources_tried.append(f"SerpApi: {len(results)} ç¯‡")
            print(f"   âœ… æ‰¾åˆ° {len(results)} ç¯‡")
        else:
            sources_tried.append(f"SerpApi: {source}")
            print(f"   âš ï¸ {source}")
    
    # å»é‡
    seen_urls = set()
    unique_results = []
    for r in all_results:
        url = r.get("url", "")
        if url and url not in seen_urls:
            seen_urls.add(url)
            unique_results.append(r)
    
    print(f"\nğŸ“Š æœç´¢ç»Ÿè®¡:")
    for s in sources_tried:
        print(f"   â€¢ {s}")
    print(f"   å»é‡å: {len(unique_results)} ç¯‡")
    
    return unique_results


def display_results(results):
    """æ˜¾ç¤ºæœç´¢ç»“æœ"""
    if not results:
        print("\næœªæ‰¾åˆ°ç»“æœ")
        return
    
    print(f"\n{'=' * 70}")
    print("æœç´¢ç»“æœ")
    print(f"{'=' * 70}")
    
    for i, r in enumerate(results[:10], 1):
        print(f"\n{i}. {r.get('title', 'Unknown')[:65]}...")
        
        authors = r.get('authors', [])
        if isinstance(authors, list):
            authors = ", ".join(authors[:3])
        if authors:
            print(f"   ä½œè€…: {authors}")
        
        if r.get('year'):
            print(f"   å¹´ä»½: {r.get('year')}")
        
        if r.get('citations'):
            print(f"   å¼•ç”¨: {r.get('citations')}")
        
        if r.get('venue'):
            print(f"   æœŸåˆŠ: {r.get('venue')}")
        
        print(f"   æ¥æº: {r.get('engine', 'unknown')}")
        
        if r.get('url'):
            print(f"   é“¾æ¥: {r.get('url')[:60]}...")
    
    print()


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Google Scholar å­¦æœ¯æœç´¢")
    parser.add_argument("--query", "-q", type=str, help="æœç´¢æŸ¥è¯¢")
    parser.add_argument("--limit", "-l", type=int, default=10, help="ç»“æœæ•°é‡")
    parser.add_argument("--serpapi", "-s", action="store_true", help="ä½¿ç”¨ SerpApi")
    parser.add_argument("--json", "-j", action="store_true", help="JSON è¾“å‡º")
    
    args = parser.parse_args()
    
    if not args.query:
        print("ç”¨æ³•: python3 google_scholar.py --query 'å…³é”®è¯'")
        print("é€‰é¡¹:")
        print("  --limit, -l  ç»“æœæ•°é‡ (é»˜è®¤ 10)")
        print("  --serpapi, -s  ä½¿ç”¨ SerpApi (éœ€è¦ API Key)")
        print("  --json, -j   JSON è¾“å‡º")
        print()
        print("ç¤ºä¾‹:")
        print("  python3 google_scholar.py --query 'transformer attention'")
        print("  python3 google_scholar.py --query 'AI' --limit 5")
        print()
        print("SerpApi é…ç½®:")
        print("  export SERPAPI_KEY='your_key'")
        print("  python3 google_scholar.py --query 'ML' --serpapi")
        return
    
    results = google_scholar_search(
        args.query,
        use_scholarly=True,
        use_serpapi=args.serpapi,
        limit=args.limit
    )
    
    if results:
        display_results(results)
        
        if args.json:
            print("\n---OUTPUT_START---")
            print(json.dumps(results, ensure_ascii=False, indent=2))
            print("---OUTPUT_END---")


if __name__ == "__main__":
    main()
