#!/usr/bin/env python3
"""
ç»¼åˆå­¦æœ¯æœç´¢å¼•æ“
èšåˆå¤šä¸ªé«˜è´¨é‡å­¦æœ¯æ•°æ®æºå’Œæœç´¢å¼•æ“
"""
import json
import time
import subprocess
from datetime import datetime
from pathlib import Path
import urllib.request
import urllib.parse
import urllib.error

# ==========================================
# å­¦æœ¯æ•°æ®æºé…ç½®
# ==========================================
ACADEMIC_SOURCES = {
    # arXiv - é¢„å°æœ¬
    "arxiv": {
        "url": "http://export.arxiv.org/api/query",
        "enabled": True,
        "weight": 2,
    },
    # Semantic Scholar - AI ä¸“ç”¨
    "semantic_scholar": {
        "url": "https://api.semanticscholar.org/graph/v1/paper/search",
        "enabled": True,
        "weight": 3,
        "params": {"fields": "title,abstract,authors,year,url,citationCount", "limit": 10},
    },
    # Crossref - æœŸåˆŠè®ºæ–‡
    "crossref": {
        "url": "https://api.crossref.org/works",
        "enabled": True,
        "weight": 2,
    },
    # PubMed - ç”Ÿç‰©åŒ»å­¦
    "pubmed": {
        "url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
        "enabled": True,
        "weight": 2,
    },
    # OpenAlex - å¼€æ”¾å­¦æœ¯
    "openalex": {
        "url": "https://api.openalex.org/works",
        "enabled": True,
        "weight": 2,
    },
    # Microsoft Academic (å·²é€€å½¹ï¼Œæ”¹ç”¨ OpenAlex)
    # ar5iv - arXiv è®ºæ–‡å…¨æ–‡
    "ar5iv": {
        "url": "https://ar5iv.org/abs",
        "enabled": True,
        "weight": 1,
    },
}

# ==========================================
# é€šç”¨æœç´¢å¼•æ“é…ç½®
# ==========================================
SEARCH_ENGINES = {
    # DuckDuckGo - éšç§æœç´¢
    "duckduckgo": {
        "url": "https://api.duckduckgo.com/",
        "enabled": True,
        "weight": 1,
    },
    # Brave Search - é«˜è´¨é‡
    "brave": {
        "url": "https://api.search.brave.com/res/v1/search",
        "enabled": False,  # éœ€è¦ API Key
        "weight": 3,
    },
    # Startpage - éšç§
    "startpage": {
        "url": "https://www.startpage.com/do/search",
        "enabled": False,
        "weight": 1,
    },
    # Bing (via SearXNG)
    "bing": {
        "url": "https://searx.be/search",
        "enabled": True,
        "weight": 2,
    },
}

# ==========================================
# æœç´¢åˆ†ç±»æ¨¡æ¿
# ==========================================
QUERY_TEMPLATES = {
    "ai_research": [
        "artificial intelligence research 2024",
        "large language model optimization",
        "AI governance ethics safety",
        "neural network architecture",
        "transformer model training",
    ],
    "machine_learning": [
        "machine learning survey 2024",
        "deep learning reinforcement learning",
        "unsupervised learning clustering",
        "natural language processing",
        "computer vision transformer",
    ],
    "data_science": [
        "data governance framework",
        "metadata management quality",
        "knowledge graph construction",
        "data pipeline automation",
    ],
    "finance": [
        "stock market prediction AI",
        "cryptocurrency analysis blockchain",
        "algorithmic trading ML",
        "financial risk assessment",
    ],
}


def search_arxiv(query, limit=10):
    """æœç´¢ arXiv é¢„å°æœ¬"""
    try:
        url = f"http://export.arxiv.org/api/query?search_query=all:{urllib.parse.quote(query)}&max_results={limit}"
        
        result = subprocess.run(
            ["curl", "-s", "--max-time", "15", url],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            import re
            entries = re.findall(r'<entry>(.*?)</entry>', result.stdout, re.DOTALL)
            
            results = []
            for entry in entries[:limit]:
                title = re.search(r'<title>(.*?)</title>', entry)
                summary = re.search(r'<summary>(.*?)</summary>', entry, re.DOTALL)
                link = re.search(r'<id>(.*?)</id>', entry)
                published = re.search(r'<published>(.*?)</published>', entry)
                authors = re.findall(r'<name>(.*?)</name>', entry)
                
                results.append({
                    "title": title.group(1).strip().replace('\n', ' ') if title else "Unknown",
                    "url": link.group(1).strip() if link else "",
                    "abstract": summary.group(1).strip().replace('\n', ' ')[:500] if summary else "",
                    "authors": authors[:3],
                    "year": published.group(1)[:4] if published else "",
                    "engine": "arXiv",
                    "citations": 0,
                })
            
            return results, "arXiv"
    except Exception as e:
        pass
    return [], "arXiv"


def search_semantic_scholar(query, limit=10):
    """æœç´¢ Semantic Scholar"""
    try:
        params = {
            "query": query,
            "fields": "title,abstract,authors,year,url,citationCount,openAccessPdf",
            "limit": limit,
        }
        url = f"https://api.semanticscholar.org/graph/v1/paper/search?{urllib.parse.urlencode(params)}"
        
        req = urllib.request.Request(
            url,
            headers={"User-Agent": "DeepSeeker/1.0"}
        )
        
        with urllib.request.urlopen(req, timeout=15) as response:
            data = json.loads(response.read().decode())
            
            results = []
            for paper in data.get("data", []):
                results.append({
                    "title": paper.get("title", ""),
                    "url": paper.get("url", ""),
                    "abstract": paper.get("abstract", "")[:500],
                    "authors": [a.get("name", "") for a in paper.get("authors", [])[:3]],
                    "year": str(paper.get("year", "")),
                    "engine": "Semantic Scholar",
                    "citations": paper.get("citationCount", 0),
                    "pdf": paper.get("openAccessPdf", {}).get("url", ""),
                })
            
            return results, "Semantic Scholar"
    except Exception as e:
        pass
    return [], "Semantic Scholar"


def search_crossref(query, limit=10):
    """æœç´¢ Crossref æœŸåˆŠ"""
    try:
        params = {
            "query": query,
            "rows": limit,
            "select": "title,author,published-print,URL,container-title",
        }
        url = f"https://api.crossref.org/works?{urllib.parse.urlencode(params)}"
        
        req = urllib.request.Request(
            url,
            headers={"User-Agent": "DeepSeeker/1.0 (mailto:research@example.com)"}
        )
        
        with urllib.request.urlopen(req, timeout=15) as response:
            data = json.loads(response.read().decode())
            
            results = []
            for item in data.get("message", {}).get("items", []):
                results.append({
                    "title": item.get("title", [""])[0] if item.get("title") else "",
                    "url": item.get("URL", ""),
                    "abstract": "",  # Crossref ä¸æä¾›æ‘˜è¦
                    "authors": [a.get("family", "") for a in item.get("author", [])[:3]],
                    "year": item.get("published-print", {}).get("date-parts", [[None]])[0][0] if item.get("published-print") else "",
                    "engine": "Crossref",
                    "journal": item.get("container-title", [""])[0] if item.get("container-title") else "",
                })
            
            return results, "Crossref"
    except Exception as e:
        pass
    return [], "Crossref"


def search_openalex(query, limit=10):
    """æœç´¢ OpenAlex å¼€æ”¾å­¦æœ¯"""
    try:
        params = {
            "search": query,
            "per-page": limit,
            "select": "id,title,abstract,authors,publication_year,host_venue,doi,open_access",
        }
        url = f"https://api.openalex.org/works?{urllib.parse.urlencode(params)}"
        
        req = urllib.request.Request(
            url,
            headers={"User-Agent": "DeepSeeker/1.0"}
        )
        
        with urllib.request.urlopen(req, timeout=15) as response:
            data = json.loads(response.read().decode())
            
            results = []
            for work in data.get("results", []):
                results.append({
                    "title": work.get("title", ""),
                    "url": work.get("doi", ""),
                    "abstract": work.get("abstract", "")[:500] if work.get("abstract") else "",
                    "authors": [a.get("author_position", "") for a in work.get("authorships", [])[:3]],
                    "year": str(work.get("publication_year", "")),
                    "engine": "OpenAlex",
                    "open_access": work.get("open_access", {}).get("is_oa", False),
                })
            
            return results, "OpenAlex"
    except Exception as e:
        pass
    return [], "OpenAlex"


def search_duckduckgo(query, limit=10):
    """DuckDuckGo é€šç”¨æœç´¢"""
    try:
        params = {
            "q": query,
            "format": "json",
            "no_html": 1,
            "skip_disambig": 1,
        }
        url = f"https://api.duckduckgo.com/?{urllib.parse.urlencode(params)}"
        
        req = urllib.request.Request(url, headers={"User-Agent": "DeepSeeker/1.0"})
        
        with urllib.request.urlopen(req, timeout=10) as response:
            data = json.loads(response.read().decode())
            
            results = []
            for item in data.get("RelatedTopics", []):
                if "FirstURL" in item:
                    results.append({
                        "title": item.get("Text", "").split(" - ")[0] if " - " in item.get("Text", "") else item.get("Text", ""),
                        "url": item.get("FirstURL", ""),
                        "content": item.get("Text", ""),
                        "engine": "DuckDuckGo",
                    })
            
            return results[:limit], "DuckDuckGo"
    except Exception as e:
        pass
    return [], "DuckDuckGo"


# ==========================================
# ä¸»æœç´¢å‡½æ•°
# ==========================================

def academic_search(query, sources=None, limit_per_source=5):
    """å­¦æœ¯æœç´¢ - å¤šæºèšåˆ"""
    print("=" * 70)
    print(f"ğŸ“š å­¦æœ¯æœç´¢: {query}")
    print("=" * 70)
    
    if sources is None:
        sources = ["semantic_scholar", "arxiv", "crossref", "openalex"]
    
    all_results = []
    source_results = {}
    
    # æœç´¢å„å­¦æœ¯æº
    search_funcs = {
        "arxiv": lambda q: search_arxiv(q, limit_per_source),
        "semantic_scholar": lambda q: search_semantic_scholar(q, limit_per_source),
        "crossref": lambda q: search_crossref(q, limit_per_source),
        "openalex": lambda q: search_openalex(q, limit_per_source),
    }
    
    for source in sources:
        if source in search_funcs and ACADEMIC_SOURCES.get(source, {}).get("enabled", False):
            print(f"ğŸ” æœç´¢ {source}...")
            
            results, engine = search_funcs[source](query)
            
            if results:
                source_results[engine] = len(results)
                all_results.extend(results)
                print(f"   âœ… æ‰¾åˆ° {len(results)} ç¯‡è®ºæ–‡")
            else:
                print(f"   âš ï¸ æœªæ‰¾åˆ°ç»“æœ")
            
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ï¼‰
    seen_titles = set()
    unique_results = []
    for r in all_results:
        title_lower = r["title"].lower()
        if title_lower not in seen_titles:
            seen_titles.add(title_lower)
            unique_results.append(r)
    
    # æŒ‰å¼•ç”¨æ•°/æƒé‡æ’åº
    def sort_key(r):
        weight = ACADEMIC_SOURCES.get(r["engine"].lower().replace(" ", "_"), {}).get("weight", 1)
        citations = r.get("citations", 0)
        return (weight, citations)
    
    unique_results.sort(key=sort_key, reverse=True)
    
    print(f"\nğŸ“Š æœç´¢ç»Ÿè®¡:")
    for engine, count in source_results.items():
        print(f"   {engine}: {count} ç¯‡")
    print(f"   å»é‡å: {len(unique_results)} ç¯‡")
    
    return unique_results[:limit_per_source * len(sources)]


def general_search(query, engines=None, limit=10):
    """é€šç”¨æœç´¢"""
    print("=" * 70)
    print(f"ğŸ” æœç´¢: {query}")
    print("=" * 70)
    
    if engines is None:
        engines = ["duckduckgo"]
    
    all_results = []
    
    for engine in engines:
        if engine in SEARCH_ENGINES and SEARCH_ENGINES[engine].get("enabled", False):
            print(f"ğŸ” æœç´¢ {engine}...")
            
            if engine == "duckduckgo":
                results, _ = search_duckduckgo(query, limit)
                all_results.extend(results)
                print(f"   âœ… æ‰¾åˆ° {len(results)} æ¡")
            
            time.sleep(0.3)
    
    print(f"\nğŸ“Š æ€»è®¡: {len(all_results)} æ¡ç»“æœ")
    
    return all_results[:limit]


def display_results(results, title="æœç´¢ç»“æœ"):
    """æ˜¾ç¤ºæœç´¢ç»“æœ"""
    print(f"\n{'=' * 70}")
    print(f"{title}")
    print(f"{'=' * 70}")
    
    for i, r in enumerate(results[:15], 1):
        print(f"\n{i}. {r.get('title', 'Unknown')[:70]}...")
        
        if r.get("authors"):
            authors = ", ".join(r["authors"][:3])
            print(f"   ä½œè€…: {authors}")
        
        if r.get("year"):
            print(f"   å¹´ä»½: {r['year']}")
        
        if r.get("citations"):
            print(f"   å¼•ç”¨: {r['citations']}")
        
        if r.get("journal"):
            print(f"   æœŸåˆŠ: {r['journal']}")
        
        print(f"   æ¥æº: {r.get('engine', 'unknown')}")
        
        if r.get("url"):
            print(f"   é“¾æ¥: {r['url'][:60]}...")
    
    print()


def save_results(query, results, search_type="academic"):
    """ä¿å­˜æœç´¢ç»“æœ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    output_dir = Path.home() / ".config" / "deepseeker" / "searches"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    filename = f"{search_type}_{timestamp}.json"
    output_file = output_dir / filename
    
    data = {
        "query": query,
        "type": search_type,
        "timestamp": timestamp,
        "count": len(results),
        "results": results,
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    return output_file


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ç»¼åˆå­¦æœ¯æœç´¢å¼•æ“")
    parser.add_argument("--query", "-q", type=str, help="æœç´¢æŸ¥è¯¢")
    parser.add_argument("--academic", "-a", action="store_true", help="å­¦æœ¯æœç´¢æ¨¡å¼")
    parser.add_argument("--finance", "-f", action="store_true", help="è´¢ç»æœç´¢æ¨¡å¼")
    parser.add_argument("--json", "-j", action="store_true", help="JSON è¾“å‡º")
    parser.add_argument("--save", "-s", action="store_true", help="ä¿å­˜ç»“æœ")
    parser.add_argument("--sources", "-S", type=str, help="æŒ‡å®šæ•°æ®æº (é€—å·åˆ†éš”)")
    
    args = parser.parse_args()
    
    if not args.query:
        print("ç”¨æ³•: python3 academic_search.py --query 'å…³é”®è¯'")
        print("é€‰é¡¹:")
        print("  --academic, -a   å­¦æœ¯æœç´¢æ¨¡å¼")
        print("  --finance, -f    è´¢ç»æœç´¢æ¨¡å¼")
        print("  --json, -j       JSON è¾“å‡º")
        print("  --save, -s       ä¿å­˜ç»“æœ")
        print("  --sources, -S    æŒ‡å®šæ•°æ®æº: arxiv,semantic_scholar,crossref,openalex")
        return
    
    # è§£ææ•°æ®æº
    sources = None
    if args.sources:
        sources = args.sources.split(",")
    
    # æ‰§è¡Œæœç´¢
    if args.academic:
        results = academic_search(args.query, sources=sources)
        search_type = "academic"
    elif args.finance:
        # è´¢ç»æœç´¢ = é€šç”¨æœç´¢ + è´¢ç»å…³é”®è¯
        results = academic_search(f"{args.query} finance stock market", sources=sources)
        search_type = "finance"
    else:
        results = general_search(args.query)
        search_type = "general"
    
    # æ˜¾ç¤ºç»“æœ
    if results:
        display_results(results, f"æœç´¢ç»“æœ: {args.query}")
    
    # JSON è¾“å‡º
    if args.json and results:
        print("\n---OUTPUT_START---")
        print(json.dumps(results, ensure_ascii=False, indent=2))
        print("---OUTPUT_END---")
    
    # ä¿å­˜ç»“æœ
    if args.save and results:
        save_results(args.query, results, search_type)


if __name__ == "__main__":
    main()
