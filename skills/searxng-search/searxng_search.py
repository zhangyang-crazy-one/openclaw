#!/usr/bin/env python3
"""
ç»¼åˆæœç´¢å¼•æ“
ä½¿ç”¨å¤šä¸ªå¤‡ç”¨æºè·å–æœç´¢ç»“æœ
"""
import json
import time
from datetime import datetime
from pathlib import Path
import urllib.request
import urllib.parse
import urllib.error

# å¤‡ç”¨æœç´¢ API
SEARCH_APIS = {
    "ddg": {
        "url": "https://api.duckduckgo.com/",
        "params": {"format": "json"},
    },
    "searx": [
        "https://searx.be",
        "https://search.bus-hit.me",
    ],
}

# å­¦æœ¯æœç´¢æº
ACADEMIC_SOURCES = [
    ("arXiv", "http://export.arxiv.org/api/query"),
    ("Semantic Scholar", "https://api.semanticscholar.org/graph/v1/paper/search"),
]

# è´¢ç»æ•°æ®æº
FINANCE_SOURCES = {
    "akshare": True,  # å·²å®‰è£…
    "baostock": True,  # å·²å®‰è£…
}


def search_duckduckgo(query):
    """DuckDuckGo æœç´¢"""
    try:
        params = {
            "q": query,
            "format": "json",
            "no_html": 1,
            "skip_disambig": 1,
        }
        url = f"{SEARCH_APIS['ddg']['url']}?{urllib.parse.urlencode(params)}"
        
        req = urllib.request.Request(url, headers={"User-Agent": "DeepSeeker/1.0"})
        
        with urllib.request.urlopen(req, timeout=8) as response:
            data = json.loads(response.read().decode())
            
            results = []
            for item in data.get("RelatedTopics", []):
                if "FirstURL" in item:
                    results.append({
                        "title": item.get("Text", "").split(" - ")[0] if " - " in item.get("Text", "") else item.get("Text", ""),
                        "url": item.get("FirstURL", ""),
                        "content": item.get("Text", ""),
                        "engine": "duckduckgo",
                    })
            
            return results
    except Exception as e:
        return None


def search_arxiv(query, limit=10):
    """arXiv å­¦æœ¯æœç´¢"""
    try:
        url = f"http://export.arxiv.org/api/query?search_query=all:{urllib.parse.quote(query)}&max_results={limit}"
        
        import subprocess
        result = subprocess.run(
            ["curl", "-s", "--max-time", "10", url],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            # ç®€å•è§£æ XML
            import re
            entries = re.findall(r'<entry>(.*?)</entry>', result.stdout, re.DOTALL)
            
            results = []
            for entry in entries[:limit]:
                title = re.search(r'<title>(.*?)</title>', entry)
                summary = re.search(r'<summary>(.*?)</summary>', entry, re.DOTALL)
                link = re.search(r'<id>(.*?)</id>', entry)
                
                results.append({
                    "title": title.group(1).strip() if title else "Unknown",
                    "url": link.group(1).strip() if link else "",
                    "content": summary.group(1).strip()[:200] if summary else "",
                    "engine": "arxiv",
                })
            
            return results
    except Exception as e:
        pass
    
    return []


def academic_search(query):
    """å­¦æœ¯æœç´¢"""
    print("=" * 60)
    print(f"ğŸ“š å­¦æœ¯æœç´¢: {query}")
    print("=" * 60)
    
    # arXiv æœç´¢
    print("ğŸ” æœç´¢ arXiv...")
    results = search_arxiv(query)
    
    if results:
        print(f"\nâœ… æ‰¾åˆ° {len(results)} æ¡ arXiv ç»“æœ:\n")
        
        for i, r in enumerate(results[:10], 1):
            print(f"{i}. {r['title'][:70]}...")
            print(f"   {r['url']}")
            print()
        
        return results
    
    print("æœªæ‰¾åˆ°ç»“æœ")
    return []


def general_search(query, limit=10):
    """ç»¼åˆæœç´¢"""
    print("=" * 60)
    print(f"ğŸ” ç»¼åˆæœç´¢: {query}")
    print("=" * 60)
    
    # DuckDuckGo æœç´¢
    print("ğŸ” æœç´¢ DuckDuckGo...")
    results = search_duckduckgo(query)
    
    if results:
        print(f"\nâœ… æ‰¾åˆ° {len(results)} æ¡ç»“æœ:\n")
        
        for i, r in enumerate(results[:limit], 1):
            print(f"{i}. {r['title'][:60]}")
            print(f"   {r['url'][:60]}...")
            print()
        
        return results
    
    print("æœªæ‰¾åˆ°ç»“æœ")
    return []


def finance_search(query):
    """è´¢ç»æœç´¢"""
    print("=" * 60)
    print(f"ğŸ’° è´¢ç»æœç´¢: {query}")
    print("=" * 60)
    
    # ä½¿ç”¨ DuckDuckGo æœç´¢è´¢ç»æ–°é—»
    finance_query = f"{query} stock market financial news"
    results = search_duckduckgo(finance_query)
    
    if results:
        print(f"\nâœ… æ‰¾åˆ° {len(results)} æ¡ç»“æœ:\n")
        
        for i, r in enumerate(results[:10], 1):
            print(f"{i}. {r['title']}")
            print(f"   {r['url'][:60]}...")
            print()
        
        return results
    
    print("æœªæ‰¾åˆ°ç»“æœ")
    return []


def save_results(query, results, search_type="general"):
    """ä¿å­˜æœç´¢ç»“æœ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    output_dir = Path.home() / ".config" / "deepseeker" / "searches"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    filename = f"{search_type}_{timestamp}.json"
    output_file = output_dir / filename
    
    data = {
        "query": query,
        "type": search_type,
        "timestamp": timestamp,
        "count": len(results),
        "results": results,
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    return output_file


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ç»¼åˆæœç´¢å¼•æ“")
    parser.add_argument("--query", "-q", type=str, help="æœç´¢æŸ¥è¯¢")
    parser.add_argument("--academic", "-a", action="store_true", help="å­¦æœ¯æœç´¢")
    parser.add_argument("--finance", "-f", action="store_true", help="è´¢ç»æœç´¢")
    parser.add_argument("--json", "-j", action="store_true", help="JSON è¾“å‡º")
    parser.add_argument("--save", "-s", action="store_true", help="ä¿å­˜ç»“æœ")
    
    args = parser.parse_args()
    
    if not args.query:
        print("ç”¨æ³•: python3 searxng_search.py --query 'å…³é”®è¯'")
        print("é€‰é¡¹: --academic (å­¦æœ¯) --finance (è´¢ç») --json --save")
        return
    
    # é€‰æ‹©æœç´¢ç±»å‹
    if args.academic:
        results = academic_search(args.query)
        search_type = "academic"
    elif args.finance:
        results = finance_search(args.query)
        search_type = "finance"
    else:
        results = general_search(args.query)
        search_type = "general"
    
    # JSON è¾“å‡º
    if args.json and results:
        print("\n---OUTPUT_START---")
        print(json.dumps(results, ensure_ascii=False, indent=2))
        print("---OUTPUT_END---")
    
    # ä¿å­˜ç»“æœ
    if args.save and results:
        save_results(args.query, results, search_type)


if __name__ == "__main__":
    main()
