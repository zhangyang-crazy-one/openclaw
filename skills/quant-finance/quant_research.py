#!/usr/bin/env python3
"""
é‡åŒ–é‡‘èå­¦æœ¯ç ”ç©¶æŠ€èƒ½
ä¸ºæŠ•èµ„å†³ç­–æä¾›å­¦æœ¯æ”¯æ’‘
"""
import json
import re
import subprocess
import time
import warnings
from datetime import datetime
from pathlib import Path
import urllib.request
import urllib.parse

warnings.filterwarnings('ignore')

# ==========================================
# æœç´¢æŸ¥è¯¢æ¨¡æ¿ - é‡åŒ–é‡‘èç»†åˆ†é¢†åŸŸ
# ==========================================
QUERY_TEMPLATES = {
    # é‡åŒ–æŠ•èµ„ç­–ç•¥
    "quant": [
        "quantitative investing factor model",
        "statistical arbitrage strategy",
        "high frequency trading algorithm",
        "momentum factor investing",
        "value factor screening",
        "smart beta ETF construction",
    ],
    # æœºå™¨å­¦ä¹ é‡åŒ–
    "ml": [
        "LSTM stock prediction",
        "transformer financial forecasting",
        "machine learning asset pricing",
        "deep reinforcement learning trading",
        "neural network volatility forecasting",
        "AI hedge fund strategy",
    ],
    # é£é™©ç®¡ç†
    "risk": [
        "value at risk VaR modeling",
        "hedging strategies derivatives",
        "tail risk portfolio protection",
        "liquidity risk assessment",
        "systemic risk detection",
        "counterparty risk analysis",
    ],
    # æŠ•èµ„ç»„åˆä¼˜åŒ–
    "portfolio": [
        "portfolio optimization machine learning",
        "mean-variance efficient frontier",
        "risk parity strategy",
        "maximum diversification ratio",
        "minimum variance portfolio",
        "factor based allocation",
    ],
    # èµ„äº§å®šä»·
    "pricing": [
        "CAPM asset pricing anomalies",
        "factor investing equity returns",
        "momentum premium stock",
        "quality factor investing",
        "low volatility anomaly",
        "ETF arbitrage pricing",
    ],
    # è¡Œä¸ºé‡‘è
    "behavior": [
        "behavioral finance market anomalies",
        "investor sentiment prediction",
        "crowd trading behavior",
        "market microstructure",
        "price discovery mechanism",
        "liquidity provision",
    ],
}

# ==========================================
# å­¦æœ¯æ•°æ®æº
# ==========================================
ACADEMIC_SOURCES = {
    "arxiv": {
        "url": "http://export.arxiv.org/api/query",
        "enabled": True,
        "weight": 2,
        "category": "quantitative finance",
    },
    "semantic_scholar": {
        "url": "https://api.semanticscholar.org/graph/v1/paper/search",
        "enabled": True,
        "weight": 3,
        "category": "Artificial Intelligence",
    },
    "crossref": {
        "url": "https://api.crossref.org/works",
        "enabled": True,
        "weight": 2,
        "category": "Finance",
    },
}

# ==========================================
# æŠ•èµ„å› å­æ˜ å°„
# ==========================================
INVESTMENT_FACTORS = {
    "momentum": {"name": "åŠ¨é‡å› å­", "description": "è¿‡å»æ”¶ç›Šé«˜çš„èµ„äº§æœªæ¥æ”¶ç›Šä¹Ÿé«˜"},
    "value": {"name": "ä»·å€¼å› å­", "description": "ä½ä¼°å€¼èµ„äº§é•¿æœŸè¡¨ç°ä¼˜äºé«˜ä¼°å€¼èµ„äº§"},
    "size": {"name": "è§„æ¨¡å› å­", "description": "å°ç›˜è‚¡é•¿æœŸæ”¶ç›Šä¼˜äºå¤§ç›˜è‚¡"},
    "low_vol": {"name": "ä½æ³¢åŠ¨å› å­", "description": "ä½æ³¢åŠ¨èµ„äº§é£é™©è°ƒæ•´åæ”¶ç›Šæ›´ä¼˜"},
    "quality": {"name": "è´¨é‡å› å­", "description": "é«˜ç›ˆåˆ©ã€é«˜å¢é•¿å…¬å¸è¡¨ç°æ›´ä¼˜"},
    "dividend": {"name": "åˆ†çº¢å› å­", "description": "é«˜åˆ†çº¢å…¬å¸æ›´ç¨³å®š"},
    "liquidity": {"name": "æµåŠ¨æ€§å› å­", "description": "æµåŠ¨æ€§å¥½çš„èµ„äº§æ›´æ˜“äº¤æ˜“"},
}


def search_arxiv(query, limit=10):
    """æœç´¢ arXiv é‡åŒ–é‡‘èè®ºæ–‡"""
    try:
        url = f"http://export.arxiv.org/api/query?search_query=all:{urllib.parse.quote(query)}&max_results={limit}"
        
        result = subprocess.run(
            ["curl", "-s", "--max-time", "15", url],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            entries = re.findall(r'<entry>(.*?)</entry>', result.stdout, re.DOTALL)
            
            results = []
            for entry in entries[:limit]:
                title = re.search(r'<title>(.*?)</title>', entry)
                summary = re.search(r'<summary>(.*?)</summary>', entry, re.DOTALL)
                link = re.search(r'<id>(.*?)</id>', entry)
                published = re.search(r'<published>(.*?)</published>', entry)
                
                if title and link:
                    results.append({
                        "title": title.group(1).strip().replace('\n', ' '),
                        "url": link.group(1).strip(),
                        "abstract": (summary.group(1).strip().replace('\n', ' ')[:300] if summary else ""),
                        "year": published.group(1)[:4] if published else "",
                        "source": "arXiv",
                        "citations": 0,
                        "keywords": extract_keywords(query),
                    })
            
            return results, "arXiv"
    except Exception as e:
        pass
    return [], "arXiv"


def search_semantic_scholar(query, limit=10):
    """æœç´¢ Semantic Scholar é‡‘èè®ºæ–‡"""
    try:
        params = {
            "query": query,
            "fields": "title,abstract,authors,year,url,citationCount,topics",
            "limit": limit,
        }
        url = f"https://api.semanticscholar.org/graph/v1/paper/search?{urllib.parse.urlencode(params)}"
        
        req = urllib.request.Request(
            url,
            headers={"User-Agent": "DeepSeeker/1.0"}
        )
        
        with urllib.request.urlopen(req, timeout=15) as response:
            data = json.loads(response.read().decode())
            
            results = []
            for paper in data.get("data", []):
                topics = paper.get("topics", [])
                topic_names = [t.get("topic", "") for t in topics[:3]]
                
                results.append({
                    "title": paper.get("title", ""),
                    "url": paper.get("url", ""),
                    "abstract": paper.get("abstract", "")[:400],
                    "year": str(paper.get("year", "")),
                    "authors": [a.get("name", "") for a in paper.get("authors", [])[:3]],
                    "source": "Semantic Scholar",
                    "citations": paper.get("citationCount", 0),
                    "topics": topic_names,
                    "keywords": extract_keywords(query),
                })
            
            return results, "Semantic Scholar"
    except Exception as e:
        pass
    return [], "Semantic Scholar"


def search_crossref_finance(query, limit=10):
    """æœç´¢ Crossref é‡‘èæœŸåˆŠ"""
    try:
        params = {
            "query": query,
            "rows": limit,
            "select": "title,author,published-print,URL,container-title,type",
            "filter": "type:journal-article",
        }
        url = f"https://api.crossref.org/works?{urllib.parse.urlencode(params)}"
        
        req = urllib.request.Request(
            url,
            headers={"User-Agent": "DeepSeeker/1.0 (mailto:research@example.com)"}
        )
        
        with urllib.request.urlopen(req, timeout=15) as response:
            data = json.loads(response.read().decode())
            
            results = []
            for item in data.get("message", {}).get("items", []):
                year = item.get("published-print", {}).get("date-parts", [[None]])[0]
                results.append({
                    "title": item.get("title", [""])[0] if item.get("title") else "",
                    "url": item.get("URL", ""),
                    "year": str(year[0]) if year else "",
                    "authors": [a.get("family", "") for a in item.get("author", [])[:3]],
                    "journal": item.get("container-title", [""])[0] if item.get("container-title") else "",
                    "source": "Crossref",
                    "citations": 0,
                    "keywords": extract_keywords(query),
                })
            
            return results, "Crossref"
    except Exception as e:
        pass
    return [], "Crossref"


def extract_keywords(query):
    """æå–å…³é”®è¯"""
    query_lower = query.lower()
    keywords = []
    
    factor_keywords = ["factor", "momentum", "value", "quality", "low volatility", "size"]
    ml_keywords = ["machine learning", "LSTM", "neural", "deep learning", "transformer"]
    quant_keywords = ["quantitative", "trading", "arbitrage", "portfolio", "optimization"]
    
    for kw in factor_keywords:
        if kw in query_lower:
            keywords.append(kw)
    
    for kw in ml_keywords:
        if kw in query_lower:
            keywords.append(kw)
    
    for kw in quant_keywords:
        if kw in query_lower:
            keywords.append(kw)
    
    return keywords if keywords else ["general"]


def quant_research(query, research_type="quant", limit_per_source=5):
    """
    é‡åŒ–é‡‘èå­¦æœ¯ç ”ç©¶ä¸»å‡½æ•°
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        research_type: ç ”ç©¶ç±»å‹ (quant/ml/risk/portfolio/pricing/behavior)
        limit_per_source: æ¯ä¸ªæ•°æ®æºçš„ç»“æœæ•°é‡
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
    
    print("=" * 80)
    print("ğŸ“Š é‡åŒ–é‡‘èå­¦æœ¯ç ”ç©¶")
    print(f"ğŸ” æŸ¥è¯¢: {query}")
    print(f"ğŸ“ ç±»å‹: {research_type}")
    print(f"â° æ—¶é—´: {timestamp}")
    print("=" * 80)
    
    # é€‰æ‹©æœç´¢æŸ¥è¯¢
    queries = [query]
    if research_type in QUERY_TEMPLATES:
        queries.extend(QUERY_TEMPLATES[research_type])
    
    # å»é‡æŸ¥è¯¢
    queries = list(dict.fromkeys(queries))[:3]
    
    all_results = []
    source_stats = {}
    
    # æœç´¢å„æ•°æ®æº
    search_funcs = {
        "arXiv": lambda q: search_arxiv(q, limit_per_source),
        "Semantic Scholar": lambda q: search_semantic_scholar(q, limit_per_source),
        "Crossref": lambda q: search_crossref_finance(q, limit_per_source),
    }
    
    for source, search_func in search_funcs.items():
        print(f"\nğŸ” æœç´¢ {source}...")
        
        source_results = []
        for q in queries:
            results, _ = search_func(q)
            source_results.extend(results)
            time.sleep(0.5)
        
        # å»é‡
        seen_urls = set()
        unique_results = []
        for r in source_results:
            url = r.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(r)
        
        if unique_results:
            all_results.extend(unique_results)
            source_stats[source] = len(unique_results)
            print(f"   âœ… {source}: {len(unique_results)} ç¯‡")
        
        time.sleep(1)
    
    # æœ€ç»ˆå»é‡å’Œæ’åº
    seen_urls = set()
    unique_results = []
    for r in all_results:
        url = r.get("url", "")
        if url and url not in seen_urls:
            seen_urls.add(url)
            # æŒ‰å¼•ç”¨æ•°æ’åº
            r["_score"] = r.get("citations", 0)
            unique_results.append(r)
    
    unique_results.sort(key=lambda x: x.get("_score", 0), reverse=True)
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š æœç´¢ç»Ÿè®¡:")
    for source, count in source_stats.items():
        print(f"   â€¢ {source}: {count} ç¯‡")
    print(f"   â€¢ å»é‡å: {len(unique_results)} ç¯‡")
    
    return unique_results


def generate_investment_recommendation(papers):
    """åŸºäºè®ºæ–‡ç”ŸæˆæŠ•èµ„å»ºè®®"""
    if not papers:
        return []
    
    recommendations = []
    
    # åˆ†æè®ºæ–‡ä¸­çš„å› å­
    factor_count = {}
    ml_count = 0
    risk_count = 0
    
    for paper in papers:
        title = paper.get("title", "").lower()
        abstract = paper.get("abstract", "").lower()
        keywords = paper.get("keywords", [])
        
        # æ£€æµ‹å› å­
        for factor, info in INVESTMENT_FACTORS.items():
            if factor in title or factor in abstract or factor in keywords:
                factor_count[factor] = factor_count.get(factor, 0) + 1
        
        # æ£€æµ‹ ML/AI
        if any(kw in title or kw in abstract for kw in ["machine learning", "neural", "AI", "deep learning", "LSTM"]):
            ml_count += 1
        
        # æ£€æµ‹é£é™©ç®¡ç†
        if any(kw in title or kw in abstract for kw in ["risk", "volatility", "hedging", "VaR"]):
            risk_count += 1
    
    # ç”Ÿæˆå»ºè®®
    recommendations.append({
        "type": "å› å­åˆ†æ",
        "content": f"å‘ç° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡",
        "factors": factor_count,
    })
    
    if ml_count > 0:
        recommendations.append({
            "type": "AI/ML é‡åŒ–",
            "content": f"{ml_count} ç¯‡ä½¿ç”¨æœºå™¨å­¦ä¹ çš„é‡åŒ–ç­–ç•¥ç ”ç©¶",
            "suggestion": "å…³æ³¨ AI å› å­åœ¨ç»„åˆä¼˜åŒ–ä¸­çš„åº”ç”¨",
        })
    
    if risk_count > 0:
        recommendations.append({
            "type": "é£é™©ç®¡ç†",
            "content": f"{risk_count} ç¯‡å…³äºé£é™©å»ºæ¨¡çš„ç ”ç©¶",
            "suggestion": "è€ƒè™‘åŠ å…¥æ³¢åŠ¨ç‡å› å­å’Œå°¾éƒ¨é£é™©å¯¹å†²",
        })
    
    # çƒ­é—¨å› å­
    if factor_count:
        top_factors = sorted(factor_count.items(), key=lambda x: x[1], reverse=True)[:3]
        recommendations.append({
            "type": "çƒ­é—¨å› å­",
            "content": "ç ”ç©¶çƒ­ç‚¹å› å­æ’å",
            "top_factors": [{"name": INVESTMENT_FACTORS.get(f, {}).get("name", f), "count": c} for f, c in top_factors],
        })
    
    return recommendations


def display_results(papers, show_recommendations=True):
    """æ˜¾ç¤ºæœç´¢ç»“æœ"""
    if not papers:
        print("\næœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
        return
    
    print(f"\n{'=' * 80}")
    print("ğŸ“„ è®ºæ–‡åˆ—è¡¨")
    print(f"{'=' * 80}")
    
    for i, paper in enumerate(papers[:15], 1):
        title = paper.get("title", "Unknown")[:70]
        year = paper.get("year", "")
        citations = paper.get("citations", 0)
        source = paper.get("source", "")
        
        print(f"\n{i}. {title}...")
        
        if year:
            print(f"   ğŸ“… {year}")
        
        if citations:
            print(f"   ğŸ“Š å¼•ç”¨: {citations}")
        
        if paper.get("authors"):
            authors = paper["authors"]
            if isinstance(authors, list):
                authors = ", ".join(authors[:2])
            print(f"   ğŸ‘¤ {authors}")
        
        if paper.get("journal"):
            print(f"   ğŸ“– {paper['journal']}")
        
        print(f"   ğŸ”— {source}")
        
        # æŠ•èµ„ç›¸å…³å…³é”®è¯
        keywords = paper.get("keywords", [])
        if keywords:
            print(f"   ğŸ·ï¸ å…³é”®è¯: {', '.join(keywords)}")
    
    # ç”ŸæˆæŠ•èµ„å»ºè®®
    if show_recommendations:
        recommendations = generate_investment_recommendation(papers)
        
        if recommendations:
            print(f"\n{'=' * 80}")
            print("ğŸ’¡ æŠ•èµ„å»ºè®®")
            print(f"{'=' * 80}")
            
            for rec in recommendations:
                print(f"\nğŸ“Œ {rec['type']}")
                print(f"   {rec['content']}")
                
                if rec.get("suggestion"):
                    print(f"   ğŸ’ å»ºè®®: {rec['suggestion']}")
                
                if rec.get("factors"):
                    print(f"   å› å­åˆ†å¸ƒ:")
                    for f, c in rec.get("factors", {}).items():
                        name = INVESTMENT_FACTORS.get(f, {}).get("name", f)
                        print(f"      â€¢ {name}: {c} ç¯‡")
                
                if rec.get("top_factors"):
                    print(f"   çƒ­é—¨:")
                    for tf in rec.get("top_factors", []):
                        print(f"      â€¢ {tf['name']}: {tf['count']} ç¯‡")


def save_results(query, papers, research_type="quant"):
    """ä¿å­˜æœç´¢ç»“æœ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    
    output_dir = Path.home() / ".config" / "deepseeker" / "quant_research"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    filename = f"{research_type}_{timestamp}.json"
    output_file = output_dir / filename
    
    recommendations = generate_investment_recommendation(papers)
    
    data = {
        "query": query,
        "type": research_type,
        "timestamp": timestamp,
        "paper_count": len(papers),
        "recommendations": recommendations,
        "papers": papers,
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    return output_file


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="é‡åŒ–é‡‘èå­¦æœ¯ç ”ç©¶")
    parser.add_argument("--query", "-q", type=str, help="æœç´¢æŸ¥è¯¢")
    parser.add_argument("--type", "-t", type=str, default="quant",
                       choices=["quant", "ml", "risk", "portfolio", "pricing", "behavior"],
                       help="ç ”ç©¶ç±»å‹")
    parser.add_argument("--limit", "-l", type=int, default=10, help="æ¯ä¸ªæ•°æ®æºçš„ç»“æœæ•°")
    parser.add_argument("--recommend", "-r", action="store_true", help="æ˜¾ç¤ºæŠ•èµ„å»ºè®®")
    parser.add_argument("--save", "-s", action="store_true", help="ä¿å­˜ç»“æœ")
    parser.add_argument("--json", "-j", action="store_true", help="JSON è¾“å‡º")
    
    args = parser.parse_args()
    
    if not args.query:
        print("ç”¨æ³•: python3 quant_research.py --query 'å…³é”®è¯'")
        print()
        print("ç ”ç©¶ç±»å‹:")
        print("  --type quant      é‡åŒ–æŠ•èµ„ç­–ç•¥ (é»˜è®¤)")
        print("  --type ml         æœºå™¨å­¦ä¹ é‡åŒ–")
        print("  --type risk       é£é™©ç®¡ç†")
        print("  --type portfolio  æŠ•èµ„ç»„åˆä¼˜åŒ–")
        print("  --type pricing    èµ„äº§å®šä»·")
        print("  --type behavior   è¡Œä¸ºé‡‘è")
        print()
        print("é€‰é¡¹:")
        print("  --recommend, -r   æ˜¾ç¤ºæŠ•èµ„å»ºè®®")
        print("  --save, -s        ä¿å­˜ç»“æœ")
        print("  --json, -j         JSON è¾“å‡º")
        print()
        print("ç¤ºä¾‹:")
        print("  python3 quant_research.py --query 'factor investing'")
        print("  python3 quant_research.py --query 'LSTM' --type ml --recommend")
        print("  python3 quant_research.py --query 'risk parity' --type portfolio --save")
        return
    
    # æ‰§è¡Œæœç´¢
    papers = quant_research(args.query, args.type, args.limit)
    
    # æ˜¾ç¤ºç»“æœ
    display_results(papers, show_recommendations=args.recommend)
    
    # JSON è¾“å‡º
    if args.json and papers:
        print("\n---OUTPUT_START---")
        print(json.dumps(papers, ensure_ascii=False, indent=2))
        print("---OUTPUT_END---")
    
    # ä¿å­˜ç»“æœ
    if args.save and papers:
        save_results(args.query, papers, args.type)


if __name__ == "__main__":
    main()
