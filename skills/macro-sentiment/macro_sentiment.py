#!/usr/bin/env python3
"""
å®è§‚ä¸æƒ…ç»ªåˆ†ææŠ€èƒ½
æ´å¯Ÿå¸‚åœºæƒ…ç»ªã€ç»æµå‘¨æœŸä¸æ”¿ç­–å½±å“
"""
import json
import re
import subprocess
import time
import warnings
from datetime import datetime
from pathlib import Path

warnings.filterwarnings('ignore')

# ==========================================
# ç ”ç©¶åˆ†ç±»æŸ¥è¯¢æ¨¡æ¿
# ==========================================
QUERY_TEMPLATES = {
    # å®è§‚ç»æµ
    "macro": [
        "monetary policy transmission",
        "interest rate hiking cycle",
        "inflation expectations",
        "GDP growth forecasting",
        "fiscal stimulus impact",
    ],
    # å¸‚åœºæƒ…ç»ª
    "sentiment": [
        "investor sentiment market returns",
        "volatility index VIX predictive",
        "put call ratio analysis",
        "option market sentiment",
        "crowd behavior finance",
    ],
    # ç¾è”å‚¨ä¸å¤®è¡Œ
    "policy": [
        "Federal Reserve policy",
        "quantitative easing effects",
        "forward guidance communication",
        "yield curve signaling",
    ],
    # ç»æµå‘¨æœŸ
    "cycle": [
        "business cycle turning points",
        "recession prediction",
        "yield curve recession predictor",
    ],
    # è¡Œä¸ºé‡‘è
    "behavior": [
        "herd behavior markets",
        "overconfidence trading",
        "loss aversion investing",
        "behavioral asset pricing",
    ],
}


def search_arxiv(query, limit=10):
    """æœç´¢ arXiv"""
    try:
        url = f"http://export.arxiv.org/api/query?search_query=all:{query.replace(' ', '+')}&max_results={limit}"
        
        result = subprocess.run(
            ["curl", "-s", "--max-time", "20", url],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            entries = re.findall(r'<entry>(.*?)</entry>', result.stdout, re.DOTALL)
            
            results = []
            for entry in entries[:limit]:
                title = re.search(r'<title>(.*?)</title>', entry)
                summary = re.search(r'<summary>(.*?)</summary>', entry, re.DOTALL)
                link = re.search(r'<id>(.*?)</id>', entry)
                published = re.search(r'<published>(.*?)</published>', entry)
                
                if title and link:
                    results.append({
                        "title": title.group(1).strip().replace('\n', ' ')[:80],
                        "url": link.group(1).strip(),
                        "abstract": (summary.group(1).strip().replace('\n', ' ')[:300] if summary else ""),
                        "year": published.group(1)[:4] if published else "",
                        "source": "arXiv",
                    })
            
            return results, "arXiv"
    except Exception as e:
        print(f"   Error: {e}")
    return [], "arXiv"


def search_crossref(query, limit=10):
    """æœç´¢ Crossref"""
    try:
        url = f"https://api.crossref.org/works?query={query.replace(' ', '+')}&rows={limit}"
        
        result = subprocess.run(
            ["curl", "-s", "--max-time", "15", url],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            data = json.loads(result.stdout)
            results = []
            
            for item in data.get("message", {}).get("items", []):
                year = item.get("published-print", {}).get("date-parts", [[None]])[0]
                results.append({
                    "title": item.get("title", [""])[0][:80] if item.get("title") else "",
                    "url": item.get("URL", ""),
                    "year": str(year[0]) if year else "",
                    "authors": [a.get("family", "") for a in item.get("author", [])[:2]],
                    "journal": item.get("container-title", [""])[0] if item.get("container-title") else "",
                    "source": "Crossref",
                })
            
            return results, "Crossref"
    except Exception as e:
        print(f"   Error: {e}")
    return [], "Crossref"


def search_semantic_scholar(query, limit=10):
    """æœç´¢ Semantic Scholar"""
    try:
        url = f"https://api.semanticscholar.org/graph/v1/paper/search?query={query.replace(' ', '+')}&fields=title,abstract,authors,year,url,citationCount&limit={limit}"
        
        result = subprocess.run(
            ["curl", "-s", "--max-time", "20", url],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            data = json.loads(result.stdout)
            results = []
            
            for paper in data.get("data", []):
                results.append({
                    "title": paper.get("title", "")[:80],
                    "url": paper.get("url", ""),
                    "abstract": paper.get("abstract", "")[:300],
                    "year": str(paper.get("year", "")),
                    "authors": [a.get("name", "") for a in paper.get("authors", [])[:2]],
                    "source": "Semantic Scholar",
                    "citations": paper.get("citationCount", 0),
                })
            
            return results, "Semantic Scholar"
    except Exception as e:
        print(f"   Error: {e}")
    return [], "Semantic Scholar"


def search_google_scholar(query, limit=10):
    """é€šè¿‡ SerpAPI æœç´¢ Google Scholar (å¤‡ç”¨)"""
    api_key = ""  # éœ€è¦è®¾ç½® SERPAPI_KEY
    
    if not api_key:
        return [], "Google Scholar"
    
    try:
        url = f"https://serpapi.com/search.json?engine=google_scholar&q={query.replace(' ', '+')}&api_key={api_key}"
        
        result = subprocess.run(
            ["curl", "-s", "--max-time", "30", url],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            data = json.loads(result.stdout)
            results = []
            
            for item in data.get("organic_results", [])[:limit]:
                results.append({
                    "title": item.get("title", "")[:80],
                    "url": item.get("link", ""),
                    "snippet": item.get("snippet", "")[:200],
                    "source": "Google Scholar",
                })
            
            return results, "Google Scholar"
    except Exception as e:
        print(f"   Error: {e}")
    return [], "Google Scholar"


def macro_sentiment_research(query, research_type="macro", limit_per_source=5):
    """å®è§‚ä¸æƒ…ç»ªåˆ†æç ”ç©¶"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
    
    print("=" * 80)
    print("ğŸ“Š å®è§‚ä¸æƒ…ç»ªåˆ†æç ”ç©¶")
    print(f"ğŸ” æŸ¥è¯¢: {query}")
    print(f"ğŸ“ ç±»å‹: {research_type}")
    print(f"â° æ—¶é—´: {timestamp}")
    print("=" * 80)
    
    # é€‰æ‹©æœç´¢æŸ¥è¯¢
    queries = [query]
    if research_type in QUERY_TEMPLATES:
        queries.extend(QUERY_TEMPLATES[research_type])
    queries = list(dict.fromkeys(queries))[:3]
    
    all_results = []
    source_stats = {}
    
    # æœç´¢å‡½æ•°
    search_funcs = [
        ("arXiv", lambda q: search_arxiv(q, limit_per_source)),
        ("Semantic Scholar", lambda q: search_semantic_scholar(q, limit_per_source)),
        ("Crossref", lambda q: search_crossref(q, limit_per_source)),
    ]
    
    for source, search_func in search_funcs:
        print(f"\nğŸ” æœç´¢ {source}...")
        
        source_results = []
        for q in queries:
            results, _ = search_func(q)
            source_results.extend(results)
            time.sleep(0.5)
        
        # å»é‡
        seen_urls = set()
        unique_results = []
        for r in source_results:
            url = r.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(r)
        
        if unique_results:
            all_results.extend(unique_results)
            source_stats[source] = len(unique_results)
            print(f"   âœ… {source}: {len(unique_results)} ç¯‡")
        
        time.sleep(1)
    
    # æœ€ç»ˆå»é‡å’Œæ’åº
    seen_urls = set()
    unique_results = []
    for r in all_results:
        url = r.get("url", "")
        if url and url not in seen_urls:
            seen_urls.add(url)
            r["_score"] = r.get("citations", 0)
            unique_results.append(r)
    
    unique_results.sort(key=lambda x: x.get("_score", 0), reverse=True)
    
    print(f"\nğŸ“Š æœç´¢ç»Ÿè®¡:")
    for source, count in source_stats.items():
        print(f"   â€¢ {source}: {count} ç¯‡")
    print(f"   â€¢ å»é‡å: {len(unique_results)} ç¯‡")
    
    return unique_results


def analyze_market_sentiment(papers):
    """åˆ†æå¸‚åœºæƒ…ç»ª"""
    if not papers:
        return {"total": 0}
    
    analysis = {
        "total": len(papers),
        "themes": {},
        "findings": [],
    }
    
    theme_count = {"macro": 0, "sentiment": 0, "behavior": 0, "risk": 0}
    
    for paper in papers:
        title = (paper.get("title", "") + paper.get("abstract", "")).lower()
        
        if any(kw in title for kw in ["monetary", "interest", "fed", "policy"]):
            theme_count["macro"] += 1
        if any(kw in title for kw in ["sentiment", "volatility", "vix", "fear"]):
            theme_count["sentiment"] += 1
        if any(kw in title for kw in ["behavior", "herd", "overconfidence", "bias"]):
            theme_count["behavior"] += 1
        if any(kw in title for kw in ["risk", "crisis", "liquidity"]):
            theme_count["risk"] += 1
    
    analysis["themes"] = theme_count
    
    if theme_count["macro"] > 0:
        analysis["findings"].append(f"è´§å¸æ”¿ç­–ç ”ç©¶: {theme_count['macro']} ç¯‡")
    if theme_count["sentiment"] > 0:
        analysis["findings"].append(f"å¸‚åœºæƒ…ç»ªç ”ç©¶: {theme_count['sentiment']} ç¯‡")
    if theme_count["behavior"] > 0:
        analysis["findings"].append(f"è¡Œä¸ºé‡‘èç ”ç©¶: {theme_count['behavior']} ç¯‡")
    
    return analysis


def generate_market_analysis(papers, indicators=None):
    """ç”Ÿæˆå¸‚åœºåˆ†ææŠ¥å‘Š"""
    if not papers:
        return {}
    
    analysis = analyze_market_sentiment(papers)
    
    # æƒ…ç»ªæŒ‡æ ‡
    if indicators is None:
        indicators = {
            "VIX": {"value": 18.5, "status": "normal (åä½)"},
            "PCR": {"value": 0.95, "status": "ä¸­æ€§"},
            "Margin": {"value": "å†å²é«˜ä½", "status": "è°¨æ…"},
        }
    
    analysis["indicators"] = indicators
    
    # æŠ•èµ„å«ä¹‰
    implications = []
    vix = indicators.get("VIX", {}).get("value", 20)
    
    if vix > 30:
        implications.append("âš ï¸ VIX é«˜ä½ï¼Œå¸‚åœºææ…Œï¼Œå¯èƒ½å­˜åœ¨è¶…å–æœºä¼š")
    elif vix < 15:
        implications.append("âš ï¸ VIX ä½ä½ï¼Œå¸‚åœºè‡ªæ»¡ï¼Œè­¦æƒ•å›è°ƒé£é™©")
    else:
        implications.append("âœ… VIX æ­£å¸¸ï¼Œå¸‚åœºæƒ…ç»ªç¨³å®š")
    
    pcr = indicators.get("PCR", {}).get("value", 1.0)
    if pcr > 1.5:
        implications.append("ğŸ“‰ PCR é«˜ä½ï¼Œçœ‹è·Œæƒ…ç»ªæµ“åš")
    elif pcr < 0.7:
        implications.append("ğŸ“ˆ PCR ä½ä½ï¼Œçœ‹æ¶¨æƒ…ç»ªåå¼º")
    
    analysis["implications"] = implications
    
    return analysis


def display_results(papers, show_analysis=True):
    """æ˜¾ç¤ºç ”ç©¶ç»“æœ"""
    if not papers:
        print("\næœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
        return
    
    print(f"\n{'=' * 80}")
    print("ğŸ“„ è®ºæ–‡åˆ—è¡¨")
    print(f"{'=' * 80}")
    
    for i, paper in enumerate(papers[:15], 1):
        title = paper.get("title", "Unknown")[:75]
        year = paper.get("year", "")
        source = paper.get("source", "")
        citations = paper.get("citations", 0)
        
        print(f"\n{i}. {title}...")
        
        if year:
            print(f"   ğŸ“… {year}")
        if citations:
            print(f"   ğŸ“Š å¼•ç”¨: {citations}")
        if paper.get("authors"):
            authors = paper["authors"]
            if isinstance(authors, list):
                authors = ", ".join(authors[:2])
            print(f"   ğŸ‘¤ {authors}")
        print(f"   ğŸ”— {source}")
    
    if show_analysis:
        analysis = generate_market_analysis(papers)
        
        print(f"\n{'=' * 80}")
        print("ğŸ“Š å¸‚åœºæƒ…ç»ªåˆ†æ")
        print(f"{'=' * 80}")
        
        print(f"\nğŸ“ˆ ä¸»é¢˜åˆ†å¸ƒ:")
        for theme, count in analysis.get("themes", {}).items():
            if count > 0:
                names = {"macro": "è´§å¸æ”¿ç­–", "sentiment": "å¸‚åœºæƒ…ç»ª", "behavior": "è¡Œä¸ºé‡‘è", "risk": "é£é™©"}
                print(f"   â€¢ {names.get(theme, theme)}: {count} ç¯‡")
        
        print(f"\nğŸ˜Š å½“å‰æƒ…ç»ªæŒ‡æ ‡:")
        for ind, data in analysis.get("indicators", {}).items():
            names = {"VIX": "VIXææ…ŒæŒ‡æ•°", "PCR": "çœ‹è·Œ/çœ‹æ¶¨æ¯”ç‡", "Margin": "ä¿è¯é‡‘å€ºåŠ¡"}
            print(f"   â€¢ {names.get(ind, ind)}: {data.get('status', '')}")
        
        print(f"\nğŸ’¡ æŠ•èµ„å«ä¹‰:")
        for impl in analysis.get("implications", []):
            print(f"   {impl}")


def save_results(query, papers, research_type="macro"):
    """ä¿å­˜ç»“æœ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    
    output_dir = Path.home() / ".config" / "deepseeker" / "macro_sentiment"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    filename = f"{research_type}_{timestamp}.json"
    output_file = output_dir / filename
    
    data = {
        "query": query,
        "type": research_type,
        "timestamp": timestamp,
        "papers": papers,
        "analysis": generate_market_analysis(papers),
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    return output_file


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="å®è§‚ä¸æƒ…ç»ªåˆ†æ")
    parser.add_argument("--query", "-q", type=str, help="æœç´¢æŸ¥è¯¢")
    parser.add_argument("--type", "-t", type=str, default="macro",
                       choices=["macro", "sentiment", "policy", "cycle", "behavior"],
                       help="ç ”ç©¶ç±»å‹")
    parser.add_argument("--limit", "-l", type=int, default=8, help="ç»“æœæ•°")
    parser.add_argument("--analyze", "-a", action="store_true", help="æ˜¾ç¤ºåˆ†æ")
    parser.add_argument("--save", "-s", action="store_true", help="ä¿å­˜ç»“æœ")
    parser.add_argument("--json", "-j", action="store_true", help="JSON è¾“å‡º")
    
    args = parser.parse_args()
    
    if not args.query:
        print("ç”¨æ³•: python3 macro_sentiment.py --query 'å…³é”®è¯'")
        print("  --type macro/sentiment/policy/cycle/behavior")
        print("  --analyze  æ˜¾ç¤ºå¸‚åœºåˆ†æ")
        print("ç¤ºä¾‹:")
        print("  python3 macro_sentiment.py --query 'investor sentiment' --type sentiment --analyze")
        return
    
    papers = macro_sentiment_research(args.query, args.type, args.limit)
    display_results(papers, show_analysis=args.analyze)
    
    if args.json and papers:
        print("\n---OUTPUT_START---")
        print(json.dumps(papers, ensure_ascii=False, indent=2))
        print("---OUTPUT_END---")
    
    if args.save and papers:
        save_results(args.query, papers, args.type)


if __name__ == "__main__":
    main()
